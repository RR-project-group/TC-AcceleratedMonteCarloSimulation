# -*- coding: utf-8 -*-
"""tpu_black_scholes.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z2EqD5vEtYrIHiG7jACYVPj-nFSKAB0t
"""

!pip install tensorflow==2.12.0

# -*- coding: utf-8 -*-
"""
TPU-optimized Black-Scholes Monte Carlo with fixed type handling
"""

import tensorflow as tf
import time
import matplotlib.pyplot as plt
from typing import List, Dict

try:
    resolver = tf.distribute.cluster_resolver.TPUClusterResolver.connect()
    tf.config.experimental_connect_to_cluster(resolver)
    tf.tpu.experimental.initialize_tpu_system(resolver)
    strategy = tf.distribute.TPUStrategy(resolver)
    print("✅ TPU initialized:", resolver.master())
    USING_TPU = True
except Exception as e:
    print("❌ TPU initialization failed:", e)
    strategy = tf.distribute.get_strategy()
    print("Using default strategy for", 'GPU' if tf.config.list_physical_devices('GPU') else 'CPU')
    USING_TPU = False

def black_scholes_mc(S0, K, T, r, sigma, num_samples, num_steps=252, dtype=tf.float32):
    """Fixed version with proper type handling"""
    dt = tf.cast(T / num_steps, dtype)
    discount = tf.exp(-r * T)

    @tf.function(jit_compile=True)
    def simulate(seeds):
        z = tf.random.stateless_normal([num_steps], seed=seeds, dtype=dtype)

        sigma_t = tf.cast(sigma, dtype)
        r_t = tf.cast(r, dtype)
        S0_t = tf.cast(S0, dtype)
        K_t = tf.cast(K, dtype)

        drift = (r_t - 0.5 * sigma_t**2) * dt
        diffusion = sigma_t * tf.sqrt(dt) * z
        log_returns = drift + diffusion
        ST = S0_t * tf.exp(tf.reduce_sum(log_returns))
        return tf.maximum(ST - K_t, 0.0)

    seeds = tf.random.stateless_uniform(
        [num_samples, 2],
        seed=[42, 42],
        minval=0,
        maxval=2**31-1,
        dtype=tf.int32
    )

    payoffs = tf.vectorized_map(simulate, seeds)
    return discount * tf.reduce_mean(payoffs)

def run_experiment(strategy, sample_sizes, precision):
    S0, K, T, r, sigma = 100.0, 120.0, 1.0, 0.05, 0.2
    dtype = tf.bfloat16 if (precision == 'bf16' and USING_TPU) else tf.float32
    results = []

    for N in sample_sizes:
        start_time = time.time()
        with strategy.scope():
            if USING_TPU and precision == 'bf16':
                from tensorflow.tpu.experimental import embed
                with embed.bfloat16_scope():
                    price = black_scholes_mc(S0, K, T, r, sigma, int(N), dtype=dtype)
            else:
                price = black_scholes_mc(S0, K, T, r, sigma, int(N), dtype=dtype)

        elapsed_ms = (time.time() - start_time) * 1000
        results.append({
            'N': int(N),
            'price': float(price.numpy()),
            'time_ms': elapsed_ms
        })
        print(f"N={N}: price={price.numpy():.4f}, time={elapsed_ms:.2f}ms")

    return results

if __name__ == "__main__":
    !pip install tensorflow==2.12.0 --quiet

    Ns = [10**3, 10**4, 10**5]

    print("\nRunning FP32 experiment...")
    results_fp32 = run_experiment(strategy, Ns, 'fp32')

    print("\nRunning BF16 experiment...")
    results_bf16 = run_experiment(strategy, Ns, 'bf16')

    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot([r['N'] for r in results_fp32], [r['price'] for r in results_fp32], 'bo-', label='FP32')
    plt.plot([r['N'] for r in results_bf16], [r['price'] for r in results_bf16], 'rx-', label='BF16')
    plt.axhline(3.247, color='k', linestyle='--', label='Analytic')
    plt.xscale('log')
    plt.xlabel('Number of Samples')
    plt.ylabel('Option Price')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot([r['N'] for r in results_fp32], [r['time_ms'] for r in results_fp32], 'bo-', label='FP32')
    plt.plot([r['N'] for r in results_bf16], [r['time_ms'] for r in results_bf16], 'rx-', label='BF16')
    plt.xscale('log')
    plt.yscale('log')
    plt.xlabel('Number of Samples')
    plt.ylabel('Time (ms)')
    plt.legend()

    plt.tight_layout()
    plt.show()